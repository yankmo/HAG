#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ„å›¾è¯†åˆ«å’ŒNeo4jæ•°æ®å¯¼å…¥æ¨¡å—
ä½¿ç”¨Ollamaçš„deepseek-r1:7bæ¨¡å‹è¿›è¡Œæ„å›¾è¯†åˆ«ï¼Œæå–å®ä½“å’Œå…³ç³»ï¼Œå¹¶å¯¼å…¥Neo4j
"""

import json
import re
import requests
from typing import Dict, List, Tuple, Any
from py2neo import Graph, Node, Relationship
from dataclasses import dataclass
import logging

from config import get_config

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class Entity:
    """å®ä½“ç±»"""
    name: str
    type: str
    properties: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.properties is None:
            self.properties = {}

@dataclass
class Relation:
    """å…³ç³»ç±»"""
    source: str
    target: str
    relation_type: str
    properties: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.properties is None:
            self.properties = {}

class OllamaClient:
    """Ollamaå®¢æˆ·ç«¯"""
    
    def __init__(self, base_url: str = None, model: str = None):
        config = get_config()
        
        self.base_url = base_url or config.ollama.base_url
        self.model = model or config.ollama.default_model
        
    def generate(self, prompt: str, system_prompt: str = None, timeout: int = 180) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        url = f"{self.base_url}/api/generate"
        
        data = {
            "model": self.model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.1,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¨³å®šçš„ç»“æœ
                "top_p": 0.9,
                "num_predict": 8192  # å¢åŠ æœ€å¤§è¾“å‡ºé•¿åº¦ä»¥é¿å…å“åº”è¢«æˆªæ–­
            }
        }
        
        if system_prompt:
            data["system"] = system_prompt
            
        try:
            response = requests.post(url, json=data, timeout=timeout)
            response.raise_for_status()
            result = response.json()
            return result.get("response", "")
        except Exception as e:
            logger.error(f"Ollamaè¯·æ±‚å¤±è´¥: {e}")
            return ""

class IntentRecognizer:
    """æ„å›¾è¯†åˆ«å™¨"""
    
    def __init__(self, ollama_client: OllamaClient):
        self.ollama = ollama_client
        
    def extract_entities_and_relations(self, text: str) -> Tuple[List[Entity], List[Relation]]:
        """ä»æ–‡æœ¬ä¸­æå–å®ä½“å’Œå…³ç³»"""
        
        # ç³»ç»Ÿæç¤ºè¯ (è‹±æ–‡ç‰ˆæœ¬ä»¥æé«˜å¤„ç†æ•ˆç‡)
        system_prompt = """You are a professional medical knowledge graph construction expert. Your task is to extract entities and relationships from medical texts to build knowledge graphs.

Please strictly follow the JSON format below for output:
{
    "entities": [
        {
            "name": "entity_name",
            "type": "entity_type",
            "properties": {"description": "entity_description"}
        }
    ],
    "relations": [
        {
            "source": "source_entity_name",
            "target": "target_entity_name", 
            "relation_type": "relationship_type",
            "properties": {"description": "relationship_description"}
        }
    ]
}

Entity types include: Disease, Symptom, Treatment, Drug, Gene, Protein, BodyPart, Cause, Risk

Relationship types include: CAUSES, TREATS, HAS_SYMPTOM, AFFECTS, RELATED_TO, LOCATED_IN, INCREASES_RISK, DECREASES_RISK, PART_OF

Please ensure the output is valid JSON format."""

        # ç”¨æˆ·æç¤ºè¯ (è‹±æ–‡ç‰ˆæœ¬)
        user_prompt = f"""Please analyze the following medical text and extract entities and relationships:

{text}

Extract key medical entities and their relationships to build a knowledge graph."""

        try:
            response = self.ollama.generate(user_prompt, system_prompt)
            logger.info(f"LLMå“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
            
            # å°è¯•è§£æJSON
            entities, relations = self._parse_llm_response(response)
            
            if not entities and not relations:
                logger.warning("æ— æ³•ä»å“åº”ä¸­æå–æœ‰æ•ˆçš„JSONæ•°æ®")
                return [], []
            
            return entities, relations
            
        except Exception as e:
            logger.error(f"å®ä½“å…³ç³»æå–å¤±è´¥: {e}")
            return [], []
    
    def _parse_llm_response(self, response: str) -> Tuple[List[Entity], List[Relation]]:
        """è§£æLLMå“åº”"""
        entities = []
        relations = []
        
        try:
            if not response:
                logger.warning("å“åº”ä¸ºç©º")
                return entities, relations
            
            # æ¸…ç†å“åº”
            cleaned_response = response.strip()
            
            # æ–¹æ³•1: ç›´æ¥è§£æJSON
            try:
                json_data = json.loads(cleaned_response)
                if "entities" in json_data or "relations" in json_data:
                    logger.info("ç›´æ¥JSONè§£ææˆåŠŸ")
                    return self._extract_entities_relations_from_json(json_data)
            except Exception as e:
                pass
            
            # æ–¹æ³•2: æŸ¥æ‰¾jsonä»£ç å—
            json_pattern = r'```json\s*(.*?)\s*```'
            matches = re.findall(json_pattern, cleaned_response, re.DOTALL)
            if matches:
                try:
                    json_str = matches[0].strip()
                    json_data = json.loads(json_str)
                    if "entities" in json_data or "relations" in json_data:
                        logger.info("jsonä»£ç å—è§£ææˆåŠŸ")
                        return self._extract_entities_relations_from_json(json_data)
                except Exception as e:
                    pass
            
            # æ–¹æ³•3: æŸ¥æ‰¾ä»»ä½•ä»£ç å—
            code_pattern = r'```\s*(.*?)\s*```'
            matches = re.findall(code_pattern, cleaned_response, re.DOTALL)
            if matches:
                for i, match in enumerate(matches):
                    try:
                        json_str = match.strip()
                        json_data = json.loads(json_str)
                        if "entities" in json_data or "relations" in json_data:
                            logger.info(f"é€šç”¨ä»£ç å—è§£ææˆåŠŸ")
                            return self._extract_entities_relations_from_json(json_data)
                    except Exception as e:
                        continue
            
            # æ–¹æ³•4: æ‰‹åŠ¨æå–JSONå¯¹è±¡
            start_idx = cleaned_response.find('{')
            if start_idx != -1:
                brace_count = 0
                for i, char in enumerate(cleaned_response[start_idx:], start_idx):
                    if char == '{':
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            try:
                                json_str = cleaned_response[start_idx:i+1]
                                json_data = json.loads(json_str)
                                if "entities" in json_data or "relations" in json_data:
                                    logger.info("æ‰‹åŠ¨JSONå¯¹è±¡æå–æˆåŠŸ")
                                    return self._extract_entities_relations_from_json(json_data)
                            except Exception as e:
                                break
            
            logger.warning("æ‰€æœ‰è§£ææ–¹æ³•éƒ½å¤±è´¥äº†")
            return entities, relations
            
        except Exception as e:
            logger.error(f"è§£æLLMå“åº”æ—¶å‡ºé”™: {e}")
            return entities, relations
    
    def _extract_entities_relations_from_json(self, json_data: dict) -> Tuple[List[Entity], List[Relation]]:
        """ä»JSONæ•°æ®ä¸­æå–å®ä½“å’Œå…³ç³»"""
        entities = []
        relations = []
        
        try:
            # è§£æå®ä½“
            entity_list = json_data.get("entities", [])
            for entity_data in entity_list:
                if isinstance(entity_data, dict):
                    entity = Entity(
                        name=entity_data.get("name", "").strip(),
                        type=entity_data.get("type", "Unknown").strip(),
                        properties=entity_data.get("properties", {})
                    )
                    if entity.name:  # åªæ·»åŠ æœ‰åç§°çš„å®ä½“
                        entities.append(entity)
            
            # è§£æå…³ç³»
            relation_list = json_data.get("relations", [])
            for relation_data in relation_list:
                if isinstance(relation_data, dict):
                    relation = Relation(
                        source=relation_data.get("source", "").strip(),
                        target=relation_data.get("target", "").strip(),
                        relation_type=relation_data.get("relation_type", "RELATED_TO").strip(),
                        properties=relation_data.get("properties", {})
                    )
                    if relation.source and relation.target:  # åªæ·»åŠ æœ‰æ•ˆçš„å…³ç³»
                        relations.append(relation)
                        
        except Exception as e:
            logger.error(f"ä»JSONæ•°æ®æå–å®ä½“å…³ç³»æ—¶å‡ºé”™: {e}")
            
        return entities, relations

class Neo4jImporter:
    """Neo4jæ•°æ®å¯¼å…¥å™¨"""
    
    def __init__(self, uri: str = None, username: str = None, password: str = None):
        config = get_config()
        
        uri = uri or config.neo4j.uri
        username = username or config.neo4j.username
        password = password or config.neo4j.password
        
        try:
            self.graph = Graph(uri, auth=(username, password))
            logger.info("Neo4jè¿æ¥æˆåŠŸ")
        except Exception as e:
            logger.error(f"Neo4jè¿æ¥å¤±è´¥: {e}")
            raise
    
    def clear_database(self):
        """æ¸…ç©ºæ•°æ®åº“"""
        try:
            self.graph.delete_all()
            logger.info("æ•°æ®åº“å·²æ¸…ç©º")
        except Exception as e:
            logger.error(f"æ¸…ç©ºæ•°æ®åº“å¤±è´¥: {e}")
    
    def import_entities_and_relations(self, entities: List[Entity], relations: List[Relation]):
        """å¯¼å…¥å®ä½“å’Œå…³ç³»"""
        try:
            # åˆ›å»ºå®ä½“èŠ‚ç‚¹
            entity_nodes = {}
            for entity in entities:
                node = Node(entity.type, name=entity.name, **entity.properties)
                self.graph.create(node)
                entity_nodes[entity.name] = node
            
            logger.info(f"åˆ›å»ºäº† {len(entities)} ä¸ªå®ä½“èŠ‚ç‚¹")
            
            # åˆ›å»ºå…³ç³»
            created_relations = 0
            for relation in relations:
                if relation.source in entity_nodes and relation.target in entity_nodes:
                    source_node = entity_nodes[relation.source]
                    target_node = entity_nodes[relation.target]
                    rel = Relationship(source_node, relation.relation_type, target_node, **relation.properties)
                    self.graph.create(rel)
                    created_relations += 1
                else:
                    logger.warning(f"å…³ç³»ä¸­çš„å®ä½“ä¸å­˜åœ¨: {relation.source} -> {relation.target}")
            
            logger.info(f"åˆ›å»ºäº† {created_relations} ä¸ªå…³ç³»")
                    
        except Exception as e:
            logger.error(f"æ•°æ®å¯¼å…¥å¤±è´¥: {e}")

class KnowledgeGraphBuilder:
    """çŸ¥è¯†å›¾è°±æ„å»ºå™¨"""
    
    def __init__(self):
        self.ollama = OllamaClient()
        self.recognizer = IntentRecognizer(self.ollama)
        self.importer = Neo4jImporter()
    
    def process_text_file(self, file_path: str, chunk_size: int = 500):
        """å¤„ç†æ–‡æœ¬æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path}")
            logger.info(f"æ–‡ä»¶å¤§å°: {len(content)} å­—ç¬¦")
            
            # æ¸…ç©ºæ•°æ®åº“
            self.importer.clear_database()
            
            # åˆ†å—å¤„ç†æ–‡æœ¬
            chunks = self._split_text(content, chunk_size)
            logger.info(f"æ–‡æœ¬åˆ†ä¸º {len(chunks)} ä¸ªå—")
            
            all_entities = []
            all_relations = []
            
            for i, chunk in enumerate(chunks):
                logger.info(f"å¤„ç†ç¬¬ {i+1}/{len(chunks)} å—")
                
                try:
                    entities, relations = self.recognizer.extract_entities_and_relations(chunk)
                    all_entities.extend(entities)
                    all_relations.extend(relations)
                    logger.info(f"å— {i+1}: æå–åˆ° {len(entities)} ä¸ªå®ä½“, {len(relations)} ä¸ªå…³ç³»")
                            
                except Exception as e:
                    logger.error(f"å¤„ç†å— {i+1} æ—¶å‡ºé”™: {e}")
                    import traceback
                    logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            
            # å»é‡
            unique_entities = self._deduplicate_entities(all_entities)
            unique_relations = self._deduplicate_relations(all_relations)
            
            logger.info(f"å»é‡å: {len(unique_entities)} ä¸ªå®ä½“, {len(unique_relations)} ä¸ªå…³ç³»")
            
            # å¯¼å…¥Neo4j
            self.importer.import_entities_and_relations(unique_entities, unique_relations)
            
            logger.info("çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ")
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
    
    def _split_text(self, text: str, chunk_size: int) -> List[str]:
        """åˆ†å‰²æ–‡æœ¬"""
        # æ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ç©ºç™½
        text = text.strip()
        
        # æŒ‰æ®µè½åˆ†å‰²ï¼Œè¿‡æ»¤æ‰ç©ºæ®µè½
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            if len(current_chunk) + len(paragraph) <= chunk_size:
                if current_chunk:
                    current_chunk += "\n\n" + paragraph
                else:
                    current_chunk = paragraph
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = paragraph
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        # è¿‡æ»¤æ‰ç©ºçš„æ–‡æœ¬å—
        chunks = [chunk for chunk in chunks if chunk.strip()]
        
        return chunks
    
    def _deduplicate_entities(self, entities: List[Entity]) -> List[Entity]:
        """å»é‡å®ä½“"""
        seen = set()
        unique_entities = []
        
        for entity in entities:
            key = (entity.name.lower(), entity.type)
            if key not in seen:
                seen.add(key)
                unique_entities.append(entity)
        
        return unique_entities
    
    def _deduplicate_relations(self, relations: List[Relation]) -> List[Relation]:
        """å»é‡å…³ç³»"""
        seen = set()
        unique_relations = []
        
        for relation in relations:
            key = (relation.source.lower(), relation.target.lower(), relation.relation_type)
            if key not in seen:
                seen.add(key)
                unique_relations.append(relation)
        
        return unique_relations

def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºçŸ¥è¯†å›¾è°±æ„å»ºå™¨
        builder = KnowledgeGraphBuilder()
        
        # å¤„ç†å¸•é‡‘æ£®æ°ç—‡æ–‡æ¡£
        file_path = "e:/Program/Project/rag-first/data/pajinsen.txt"
        builder.process_text_file(file_path)
        
        print("âœ… æ„å›¾è¯†åˆ«å’Œæ•°æ®å¯¼å…¥å®Œæˆï¼")
        print("ğŸ“Š è¯·åœ¨Neo4j Browserä¸­æŸ¥çœ‹æ„å»ºçš„çŸ¥è¯†å›¾è°±")
        print("ğŸ”— Neo4j Browser: http://localhost:7474")
        
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")

if __name__ == "__main__":
    main()